
@article{He2016,
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
	archivePrefix = {arXiv},
	arxivId = {1512.03385},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	doi = {10.1109/CVPR.2016.90},
	eprint = {1512.03385},
	file = {:C$\backslash$:/Users/C40-05/Desktop/MAESTRIA/MATERIAS/2017/SEMESTRE2/ProyectosFinales/Proyecto{\_}PatternRecognition/Technical{\_}Report/references/He{\_}Deep{\_}Residual{\_}Learning{\_}CVPR{\_}2016{\_}paper (1).pdf:pdf},
	isbn = {978-1-4673-8851-1},
	issn = {1664-1078},
	journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	mendeley-groups = {RESNETS},
	pages = {770--778},
	pmid = {23554596},
	title = {{Deep Residual Learning for Image Recognition}},
	url = {http://ieeexplore.ieee.org/document/7780459/},
	year = {2016}
}

@article{Veit2016,
	abstract = {In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.},
	archivePrefix = {arXiv},
	arxivId = {1605.06431},
	author = {Veit, Andreas and Wilber, Michael and Belongie, Serge},
	eprint = {1605.06431},
	file = {:C$\backslash$:/Users/C40-05/Desktop/MAESTRIA/MATERIAS/2017/SEMESTRE2/ProyectosFinales/Proyecto{\_}PatternRecognition/Technical{\_}Report/references/1605.06431.pdf:pdf},
	issn = {10495258},
	mendeley-groups = {RESNETS},
	pages = {1--9},
	title = {{Residual Networks Behave Like Ensembles of Relatively Shallow Networks}},
	url = {http://arxiv.org/abs/1605.06431},
	year = {2016}
}
@article{Wu2017,
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	archivePrefix = {arXiv},
	arxivId = {1512.03385},
	author = {Wu, Songtao and Zhong, Shenghua and Liu, Yan},
	doi = {10.1007/s11042-017-4440-4},
	eprint = {1512.03385},
	file = {:C$\backslash$:/Users/C40-05/Desktop/MAESTRIA/MATERIAS/2017/SEMESTRE2/ProyectosFinales/Proyecto{\_}PatternRecognition/Technical{\_}Report/references/1512.03385.pdf:pdf},
	isbn = {978-1-4673-6964-0},
	issn = {15737721},
	journal = {Multimedia Tools and Applications},
	keywords = {Convolutional neural networks,Image steganalysis,Residual learning},
	mendeley-groups = {RESNETS},
	pages = {1--17},
	pmid = {23554596},
	title = {{Deep residual learning for image steganalysis}},
	year = {2017}
}

@article{Xie2016,
	abstract = {We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.},
	archivePrefix = {arXiv},
	arxivId = {1611.05431},
	author = {Xie, Saining and Girshick, Ross and Doll{\'{a}}r, Piotr and Tu, Zhuowen and He, Kaiming},
	doi = {10.1109/CVPR.2017.634},
	eprint = {1611.05431},
	file = {:C$\backslash$:/Users/C40-05/Desktop/MAESTRIA/MATERIAS/2017/SEMESTRE2/ProyectosFinales/Proyecto{\_}PatternRecognition/Technical{\_}Report/references/1611.05431.pdf:pdf},
	isbn = {978-1-5386-0457-1},
	mendeley-groups = {RESNETS},
	title = {{Aggregated Residual Transformations for Deep Neural Networks}},
	url = {http://arxiv.org/abs/1611.05431},
	year = {2016}
}


@misc{Michael:Online,
	author = {Michael Dietz},
	title = {Understand Deep Residual Networks — a simple, modular learning framework that has redefined state-of-the-art},
	year = {2017},
	howpublished = "{https://blog.waya.ai/deep-residual-learning-9610bb62c355}",
	urldate = {16-12-2017}
}



@article{Zagoruyko2016,
	abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https://github.com/szagoruyko/wide-residual-networks},
	archivePrefix = {arXiv},
	arxivId = {1605.07146},
	author = {Zagoruyko, Sergey and Komodakis, Nikos},
	doi = {10.5244/C.30.87},
	eprint = {1605.07146},
	file = {:C$\backslash$:/Users/C40-05/Desktop/MAESTRIA/MATERIAS/2017/SEMESTRE2/ProyectosFinales/Proyecto{\_}PatternRecognition/Technical{\_}Report/references/1605.07146.pdf:pdf},
	isbn = {1-901725-59-6},
	mendeley-groups = {RESNETS},
	title = {{Wide Residual Networks}},
	url = {http://arxiv.org/abs/1605.07146},
	year = {2016}
}

