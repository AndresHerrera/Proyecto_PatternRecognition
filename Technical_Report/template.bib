
@article{He2016,
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
	archivePrefix = {arXiv},
	arxivId = {1512.03385},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	doi = {10.1109/CVPR.2016.90},
	eprint = {1512.03385},
	file = {:C$\backslash$:/Users/C40-05/Desktop/MAESTRIA/MATERIAS/2017/SEMESTRE2/ProyectosFinales/Proyecto{\_}PatternRecognition/Technical{\_}Report/references/He{\_}Deep{\_}Residual{\_}Learning{\_}CVPR{\_}2016{\_}paper (1).pdf:pdf},
	isbn = {978-1-4673-8851-1},
	issn = {1664-1078},
	journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	mendeley-groups = {RESNETS},
	pages = {770--778},
	pmid = {23554596},
	title = {{Deep Residual Learning for Image Recognition}},
	url = {http://ieeexplore.ieee.org/document/7780459/},
	year = {2016}
}

@article{Veit2016,
	abstract = {In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.},
	archivePrefix = {arXiv},
	arxivId = {1605.06431},
	author = {Veit, Andreas and Wilber, Michael and Belongie, Serge},
	eprint = {1605.06431},
	file = {:C$\backslash$:/Users/C40-05/Desktop/MAESTRIA/MATERIAS/2017/SEMESTRE2/ProyectosFinales/Proyecto{\_}PatternRecognition/Technical{\_}Report/references/1605.06431.pdf:pdf},
	issn = {10495258},
	mendeley-groups = {RESNETS},
	pages = {1--9},
	title = {{Residual Networks Behave Like Ensembles of Relatively Shallow Networks}},
	url = {http://arxiv.org/abs/1605.06431},
	year = {2016}
}
@article{Wu2017,
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	archivePrefix = {arXiv},
	arxivId = {1512.03385},
	author = {Wu, Songtao and Zhong, Shenghua and Liu, Yan},
	doi = {10.1007/s11042-017-4440-4},
	eprint = {1512.03385},
	file = {:C$\backslash$:/Users/C40-05/Desktop/MAESTRIA/MATERIAS/2017/SEMESTRE2/ProyectosFinales/Proyecto{\_}PatternRecognition/Technical{\_}Report/references/1512.03385.pdf:pdf},
	isbn = {978-1-4673-6964-0},
	issn = {15737721},
	journal = {Multimedia Tools and Applications},
	keywords = {Convolutional neural networks,Image steganalysis,Residual learning},
	mendeley-groups = {RESNETS},
	pages = {1--17},
	pmid = {23554596},
	title = {{Deep residual learning for image steganalysis}},
	year = {2017}
}

@article{Xie2016,
	abstract = {We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.},
	archivePrefix = {arXiv},
	arxivId = {1611.05431},
	author = {Xie, Saining and Girshick, Ross and Doll{\'{a}}r, Piotr and Tu, Zhuowen and He, Kaiming},
	doi = {10.1109/CVPR.2017.634},
	eprint = {1611.05431},
	file = {:C$\backslash$:/Users/C40-05/Desktop/MAESTRIA/MATERIAS/2017/SEMESTRE2/ProyectosFinales/Proyecto{\_}PatternRecognition/Technical{\_}Report/references/1611.05431.pdf:pdf},
	isbn = {978-1-5386-0457-1},
	mendeley-groups = {RESNETS},
	title = {{Aggregated Residual Transformations for Deep Neural Networks}},
	url = {http://arxiv.org/abs/1611.05431},
	year = {2016}
}


@misc{Michael:Online,
	author = {Michael Dietz},
	title = {Understand Deep Residual Networks — a simple, modular learning framework that has redefined state-of-the-art},
	year = {2017},
	howpublished = "{https://blog.waya.ai/deep-residual-learning-9610bb62c355}",
	urldate = {16-12-2017}
}



@article{Zagoruyko2016,
	abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https://github.com/szagoruyko/wide-residual-networks},
	archivePrefix = {arXiv},
	arxivId = {1605.07146},
	author = {Zagoruyko, Sergey and Komodakis, Nikos},
	doi = {10.5244/C.30.87},
	eprint = {1605.07146},
	file = {:C$\backslash$:/Users/C40-05/Desktop/MAESTRIA/MATERIAS/2017/SEMESTRE2/ProyectosFinales/Proyecto{\_}PatternRecognition/Technical{\_}Report/references/1605.07146.pdf:pdf},
	isbn = {1-901725-59-6},
	mendeley-groups = {RESNETS},
	title = {{Wide Residual Networks}},
	url = {http://arxiv.org/abs/1605.07146},
	year = {2016}
}

@article{Long2016,
	abstract = {The recent success of deep neural networks relies on massive amounts of labeled data. For a target task where labeled data is unavailable, domain adaptation can transfer a learner from a different source domain. In this paper, we propose a new approach to domain adaptation in deep networks that can jointly learn adaptive classifiers and transferable features from labeled data in the source domain and unlabeled data in the target domain. We relax a shared-classifier assumption made by previous methods and assume that the source classifier and target classifier differ by a residual function. We enable classifier adaptation by plugging several layers into deep network to explicitly learn the residual function with reference to the target classifier. We fuse features of multiple layers with tensor product and embed them into reproducing kernel Hilbert spaces to match distributions for feature adaptation. The adaptation can be achieved in most feed-forward models by extending them with new residual layers and loss functions, which can be trained efficiently via back-propagation. Empirical evidence shows that the new approach outperforms state of the art methods on standard domain adaptation benchmarks.},
	archivePrefix = {arXiv},
	arxivId = {1602.04433},
	author = {Long, Mingsheng and Zhu, Han and Wang, Jianmin and Jordan, Michael I.},
	eprint = {1602.04433},
	file = {:C$\backslash$:/Users/C40-05/Desktop/MAESTRIA/MATERIAS/2017/SEMESTRE2/ProyectosFinales/Proyecto{\_}PatternRecognition/Technical{\_}Report/references/6110-unsupervised-domain-adaptation-with-residual-transfer-networks.pdf:pdf},
	issn = {10495258},
	mendeley-groups = {RESNETS},
	number = {Nips},
	pmid = {199870},
	title = {{Unsupervised Domain Adaptation with Residual Transfer Networks}},
	url = {http://arxiv.org/abs/1602.04433},
	year = {2016}
}


@article{Huang2016,
	abstract = {Very deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91{\%} on CIFAR-10).},
	archivePrefix = {arXiv},
	arxivId = {1603.09382},
	author = {Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian},
	doi = {10.1007/978-3-319-46493-0_39},
	eprint = {1603.09382},
	file = {:C$\backslash$:/Users/C40-05/Desktop/MAESTRIA/MATERIAS/2017/SEMESTRE2/ProyectosFinales/Proyecto{\_}PatternRecognition/Technical{\_}Report/references/1603.09382.pdf:pdf},
	isbn = {9783319464930},
	issn = {0302-9743},
	mendeley-groups = {RESNETS},
	pmid = {4520227},
	title = {{Deep Networks with Stochastic Depth}},
	url = {http://arxiv.org/abs/1603.09382},
	year = {2016}
}

